{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25bcdf6c-3c96-48f6-b7be-3a3721d3a161",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Prerequisites for spark and arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e596d735-c5b3-4b7f-a8b8-a176fc324607",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import LongType, StructField, StructType, DoubleType, IntegerType\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a99a81-1866-431f-9310-00be53ea3a05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/02 09:40:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# setup for local testing - comment in case of databricks\n",
    "builder = SparkSession.builder.master(\"local[4]\").appName(\"pandas-on-spark\")\n",
    "builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.ui.enabled\", \"false\")\n",
    "# Pandas API on Spark automatically uses this Spark session with the configurations set.\n",
    "spark = builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3411e91f-32d2-456c-9766-7e2f722954ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Scalar UDFS vs pandas UDFs \n",
    "\n",
    "see [this post](https://gist.github.com/BryanCutler/0b0c820c1beb5ffc40618c462912195f) for more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5089a24-18aa-4b90-9a38-3ab97595dc13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Short excurse to pandas vs python udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc77cfa8-b594-4e43-94c5-86d688d41282",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.range(1 << 24, numPartitions=16).toDF(\"id\") \\\n",
    "        .withColumn(\"p1\", F.rand()).withColumn(\"p2\", F.rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "577476bf-36e7-488b-a3a1-b4cd13ee11ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "\n",
    "def scalar_func(p1, p2):\n",
    "    w = 0.5\n",
    "    return exp(log(p1) + log(p2) - log(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1a596c-268b-474e-acd8-d458ed2e5d3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:==========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.15 s ± 22.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "my_udf = F.udf(scalar_func, DoubleType())\n",
    "\n",
    "result = df.withColumn(\"p\", my_udf(F.col(\"p1\"), F.col(\"p2\")))\n",
    "\n",
    "%timeit result.filter(\"p < 1.0\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460e8fa9-17c3-4f7f-9c71-27bd4c863581",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def vect_func(p1, p2):\n",
    "    w = 0.5\n",
    "    return np.exp(np.log(p1) + np.log(p2) - np.log(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f12a5092-820f-453e-be46-00c2b2246099",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:==========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23 s ± 18.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "my_udf = F.pandas_udf(vect_func, DoubleType())\n",
    "\n",
    "result = df.withColumn(\"p\", my_udf(F.col(\"p1\"), F.col(\"p2\")))\n",
    "\n",
    "%timeit result.filter(\"p < 1.0\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "\n",
    "def scalar_func(p1, p2):\n",
    "    w = 0.5\n",
    "    return exp(log(p1) + log(p2) - log(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing a `transform` or `apply` statement differs for ps Series and ps DataFrames:\n",
    "- For [Series](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.Series.apply.html) the function is applied elementwise\n",
    "- For [DataFrames](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.transform.html) each function takes a pandas Series, and the pandas API on Spark computes the functions in a distributed manner (see (transform_apply[https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/transform_apply.html])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_ps(x: ps.Series) -> np.int64:\n",
    "    return x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max(x: np.int64) -> np.float64:\n",
    "    return np.max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    1.0\n",
       "2    2.0\n",
       "3    3.0\n",
       "4    4.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.Series(range(10000)).apply(get_max).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2499\n",
       "1    4999\n",
       "2    7499\n",
       "3    9999\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.DataFrame({'A': range(10000)})[['A']].apply(get_max_ps).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is this done with [panda udfs](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.pandas_udf.html)?\n",
    "\n",
    "- Note that the type hint should use pandas.Series in all cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(\"int\")\n",
    "def get_max_pd(x: pd.Series) -> np.int64:\n",
    "    return x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = spark.createDataFrame(pd.DataFrame({'A': range(100000)}))\n",
    "sdf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|get_max_pd(A)|\n",
      "+-------------+\n",
      "|        99999|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select(get_max_pd(F.col(\"A\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "pyspark-pandas",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
