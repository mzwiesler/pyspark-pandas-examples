{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25bcdf6c-3c96-48f6-b7be-3a3721d3a161",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Prerequisites for spark and arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e596d735-c5b3-4b7f-a8b8-a176fc324607",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import LongType, StructField, StructType, DoubleType, IntegerType\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a99a81-1866-431f-9310-00be53ea3a05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/01 17:34:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# setup for local testing - comment in case of databricks\n",
    "builder = SparkSession.builder.master(\"local[4]\").appName(\"pandas-on-spark\")\n",
    "builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.ui.enabled\", \"false\")\n",
    "# Pandas API on Spark automatically uses this Spark session with the configurations set.\n",
    "spark = builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a21678d-1c8f-4dfe-a704-dfda9f9f4ad7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LENGTH = 500000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97616ae6-2299-47fa-8489-d3cfac9ed4db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Evaluate how to get pypsark schema for pandas pyspark dataframes\n",
    "\n",
    "Note that DataFrame.spark.schema() is not listed in the [API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.transform.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc036a9-0269-49d1-8d91-59710b445c6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "psdf = ps.DataFrame({\"id\": range(LENGTH), \"value\": range(10, LENGTH+10), \"a\": range(LENGTH,LENGTH*2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09036537-804e-445c-8ad3-84416f8a6861",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('id', LongType(), False), StructField('value', LongType(), False), StructField('a', LongType(), False)])\n"
     ]
    }
   ],
   "source": [
    "print(psdf.spark.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4efd813-d74b-4372-addb-e0ed7bc6a019",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- value: long (nullable = false)\n",
      " |-- a: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "sdf = psdf.to_spark()\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d3a905c-2d80-4dc3-b988-d5bc1debf23e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Test how to enforce nullable on schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "329973e3-f415-40a6-a526-71b3e04fe578",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_new = StructType(\n",
    "    [StructField(\"id\", LongType(), True), StructField(\"value\", LongType(), True), StructField('a', LongType(), False)]\n",
    ")\n",
    "sdf_new_schema = spark.createDataFrame(sdf.rdd, schema_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d8086f-edd6-4264-8ffc-c05a25467013",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      " |-- a: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_new_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fafb5b5-89c4-4c99-b0b3-72ec4463447a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', LongType(), True), StructField('value', LongType(), True), StructField('a', LongType(), False)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf_new_schema = sdf_new_schema.pandas_api()\n",
    "psdf_new_schema.spark.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e1e8c2b-785f-436a-9642-2ec4eb356ff3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# psdf_new_schema.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03105993-e1d2-48b3-b41c-27bf17ed5127",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Select and filter columns to pandas on pyspark df\n",
    "\n",
    "The goal is to compare the performance of three different approaches: native pyspark, pandas on pyspark with vectorized function and pyspark with scalar function. For this we do the same transformation in each approach and check the execution plan as well as the time taken to execute (on a larger dataset on databricks).\n",
    "\n",
    "The transformations should be two filters which could be optimized and one complex filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaca0007-f2b8-49f0-b6c2-268d5b27c7fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First we generally investigate how pandas dataframes are working in pyspark and see that we need to be careful how functions are applied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d13f507-c776-4975-8ba2-f8e262449e0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If the type hints is not specified for `apply`, it is expensive to infer the data type internally.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "A     249\n",
       "A     499\n",
       "A     749\n",
       "A    1000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.DataFrame({'A': range(1001)}).apply(lambda col: col.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3411e91f-32d2-456c-9766-7e2f722954ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Scalar UDFS vs pandas UDFs \n",
    "\n",
    "see [this post](https://gist.github.com/BryanCutler/0b0c820c1beb5ffc40618c462912195f) for more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5089a24-18aa-4b90-9a38-3ab97595dc13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Short excurse to pandas vs python udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc77cfa8-b594-4e43-94c5-86d688d41282",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.range(1 << 24, numPartitions=16).toDF(\"id\") \\\n",
    "        .withColumn(\"p1\", F.rand()).withColumn(\"p2\", F.rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "577476bf-36e7-488b-a3a1-b4cd13ee11ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "\n",
    "def scalar_func(p1, p2):\n",
    "    w = 0.5\n",
    "    return exp(log(p1) + log(p2) - log(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1a596c-268b-474e-acd8-d458ed2e5d3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:==========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.29 s ± 17.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "my_udf = F.udf(scalar_func, DoubleType())\n",
    "\n",
    "result = df.withColumn(\"p\", my_udf(F.col(\"p1\"), F.col(\"p2\")))\n",
    "\n",
    "%timeit result.filter(\"p < 1.0\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460e8fa9-17c3-4f7f-9c71-27bd4c863581",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def vect_func(p1, p2):\n",
    "    w = 0.5\n",
    "    return np.exp(np.log(p1) + np.log(p2) - np.log(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f12a5092-820f-453e-be46-00c2b2246099",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:==========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.36 s ± 29.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "my_udf = F.pandas_udf(vect_func, DoubleType())\n",
    "\n",
    "result = df.withColumn(\"p\", my_udf(F.col(\"p1\"), F.col(\"p2\")))\n",
    "\n",
    "%timeit result.filter(\"p < 1.0\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6843ea0a-17fe-4c3b-9a4a-5493aa8785e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Start with execution of different when statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe8f42d8-96d8-4a9c-b63d-995116762c43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# scalar udf\n",
    "def scalar_when(x: int) -> int: \n",
    "    return 100 if x < LENGTH/20 else (50 if x < LENGTH/2 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb12bcb1-171d-4219-a35b-5a881fdedb72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# vectorized udf returning a pandas Series\n",
    "def vectorized_when_pd(x: pd.Series) -> pd.Series: \n",
    "  return pd.Series(np.where(x < LENGTH/20, 100, np.where(x < LENGTH/2, 50, 0)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized udf returning a numpy array\n",
    "def vectorized_when(x): \n",
    "  return np.where(x < LENGTH/20, 100, np.where(x < LENGTH/2, 50, 0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b269450-afa2-4736-aa68-06083c05748f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_dataset(len: int = LENGTH) -> ps.DataFrame:\n",
    "  return ps.DataFrame({\"id\": range(len), \"value\": range(10, len+10), \"a\": range(len,len*2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f97f873-ab51-456c-b668-23bd757cbe66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### With native pyspark pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe678b3-3c9c-434e-a5fb-fcb12efae8f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "psdf_native = generate_dataset()\n",
    "psdf_native = psdf_native.loc[psdf_native['id'] < (LENGTH - LENGTH/10)]\n",
    "psdf_native[\"new_value\"] = psdf_native[\"id\"].where(psdf_native[\"id\"] < LENGTH/20, 100).where(psdf_native[\"id\"] < LENGTH/2, 50).where(psdf_native[\"id\"] >= LENGTH/2, 0)\n",
    "psdf_native = psdf_native.loc[psdf_native['id'] < (LENGTH - LENGTH/5)]\n",
    "psdf_native = psdf_native.loc[:, ['new_value', 'id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef8683f3-9d5d-456c-b07e-0ba2e259bd9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "LocalTableScan (1)\n",
      "\n",
      "\n",
      "(1) LocalTableScan\n",
      "Output [3]: [__index_level_0__#287L, new_value#340L, id#288L]\n",
      "Arguments: [__index_level_0__#287L, new_value#340L, id#288L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "psdf_native.spark.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c92c7631-7b20-410d-ae6f-987621190008",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/01 17:35:06 WARN TaskSetManager: Stage 51 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:35:08 WARN TaskSetManager: Stage 54 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:35:09 WARN TaskSetManager: Stage 57 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:35:11 WARN TaskSetManager: Stage 60 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:35:12 WARN TaskSetManager: Stage 63 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:35:14 WARN TaskSetManager: Stage 66 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:35:15 WARN TaskSetManager: Stage 69 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5 s ± 31.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/01 17:35:17 WARN TaskSetManager: Stage 72 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "%timeit psdf_native.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35250b63-c7fc-43fb-a8de-551ccb99ff54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Use pandas (vectorized) udf for pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6773d0-1e95-4771-9e5b-09c3c6295f0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The [apply and transform](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/transform_apply.html) functions are working on pandas series and thus have the same effect as using pandas udfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8c02d53-a36c-4768-b648-2d728f676003",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Type <class 'pandas.core.series.Series'> was not understood.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/typedef/typehints.py:335\u001b[0m, in \u001b[0;36mpandas_on_spark_type\u001b[0;34m(tpe)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtpe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     spark_type \u001b[38;5;241m=\u001b[39m as_spark_type(dtype)\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1666\u001b[0m, in \u001b[0;36mpandas_dtype\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m npdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1666\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not understood\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m npdtype\n",
      "\u001b[0;31mTypeError\u001b[0m: dtype '<class 'pandas.core.series.Series'>' not understood",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m psdf_scalar \u001b[38;5;241m=\u001b[39m generate_dataset()\n\u001b[1;32m      4\u001b[0m psdf_scalar \u001b[38;5;241m=\u001b[39m psdf_scalar\u001b[38;5;241m.\u001b[39mloc[psdf_scalar[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m (LENGTH \u001b[38;5;241m-\u001b[39m LENGTH\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m)]\n\u001b[0;32m----> 5\u001b[0m psdf_scalar \u001b[38;5;241m=\u001b[39m psdf_scalar\u001b[38;5;241m.\u001b[39massign(new_value\u001b[38;5;241m=\u001b[39m\u001b[43mpsdf_scalar\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvect_when_udf\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m psdf_scalar \u001b[38;5;241m=\u001b[39m psdf_scalar\u001b[38;5;241m.\u001b[39mloc[psdf_scalar[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m (LENGTH \u001b[38;5;241m-\u001b[39m LENGTH\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m5\u001b[39m)]\n\u001b[1;32m      7\u001b[0m psdf_scalar \u001b[38;5;241m=\u001b[39m psdf_scalar\u001b[38;5;241m.\u001b[39mloc[:, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_value\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/series.py:3973\u001b[0m, in \u001b[0;36mSeries.transform\u001b[0;34m(self, func, axis, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(internal)\n\u001b[1;32m   3972\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/series.py:3803\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, args, **kwds)\u001b[0m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpandas_on_spark\u001b[38;5;241m.\u001b[39m_transform_batch(apply_each, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     sig_return \u001b[38;5;241m=\u001b[39m \u001b[43minfer_return_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sig_return, ScalarType):\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3806\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected the return type of this function to be of scalar type, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3807\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(sig_return)\n\u001b[1;32m   3808\u001b[0m         )\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/typedef/typehints.py:634\u001b[0m, in \u001b[0;36minfer_return_type\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    622\u001b[0m         data_fields\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    623\u001b[0m             InternalField(\n\u001b[1;32m    624\u001b[0m                 dtype\u001b[38;5;241m=\u001b[39mdata_dtype,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    629\u001b[0m             )\n\u001b[1;32m    630\u001b[0m         )\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameType(index_fields\u001b[38;5;241m=\u001b[39mindex_fields, data_fields\u001b[38;5;241m=\u001b[39mdata_fields)\n\u001b[0;32m--> 634\u001b[0m tpes \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_on_spark_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtpe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UnknownType(tpe)\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/typedef/typehints.py:338\u001b[0m, in \u001b[0;36mpandas_on_spark_type\u001b[0;34m(tpe)\u001b[0m\n\u001b[1;32m    336\u001b[0m     spark_type \u001b[38;5;241m=\u001b[39m as_spark_type(dtype)\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m--> 338\u001b[0m     spark_type \u001b[38;5;241m=\u001b[39m \u001b[43mas_spark_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtpe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m spark_type_to_pandas_dtype(spark_type)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dtype, spark_type\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/typedef/typehints.py:248\u001b[0m, in \u001b[0;36mas_spark_type\u001b[0;34m(tpe, raise_error, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m types\u001b[38;5;241m.\u001b[39mDoubleType()\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m was not understood.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m tpe)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Type <class 'pandas.core.series.Series'> was not understood."
     ]
    }
   ],
   "source": [
    "# this runs a spark job - in case output type is not defined - since: this API executes the function once to infer the type which is potentially expensive, for instance, when the dataset is created after aggregations or sorting. (https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.transform.html)\n",
    "\n",
    "psdf_scalar = generate_dataset()\n",
    "psdf_scalar = psdf_scalar.loc[psdf_scalar['id'] < (LENGTH - LENGTH/10)]\n",
    "psdf_scalar = psdf_scalar.assign(new_value=psdf_scalar[\"id\"].transform(vectorized_when))\n",
    "psdf_scalar = psdf_scalar.loc[psdf_scalar['id'] < (LENGTH - LENGTH/5)]\n",
    "psdf_scalar = psdf_scalar.loc[:, ['new_value', 'id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf6acda-9a04-415e-9f20-fa899a6e3c6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "psdf_scalar.spark.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6e3355a-509e-4fa3-aa0a-2c17715795c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%timeit psdf_scalar.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb78c89-88d9-475d-82c9-3f9579b8a509",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Use pandas (scalar) udf for pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1a26b97-d01d-4778-bf4c-45290fd881ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "psdf_vect = generate_dataset()\n",
    "psdf_vect = psdf_vect.loc[psdf_vect['id'] < (LENGTH - LENGTH/10)]\n",
    "psdf_vect = psdf_vect.assign(new_value=psdf_vect[\"id\"].transform(scalar_when))\n",
    "psdf_vect = psdf_vect.loc[psdf_vect['id'] < (LENGTH - LENGTH/5)]\n",
    "psdf_vect = psdf_vect.loc[:, ['new_value', 'id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f0bf293-8eac-4b23-aa71-95056bb04240",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "psdf_vect.spark.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f101cd1-67ab-4b9f-abcb-3da8db279baf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%timeit psdf_vect.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad65c3b6-7193-4272-8615-a6c84f583a90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Native pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47194588-7f80-420d-b745-e63432619a01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sdf_native = generate_dataset().to_spark()\n",
    "sdf_native = sdf_native.filter(F.col(\"id\") < LENGTH - LENGTH/10)\n",
    "sdf_native = sdf_native.withColumn(\"new_value\", F.when(F.col(\"id\") < 40, 100).when(F.col(\"id\") < 60, 50).otherwise(0))\n",
    "sdf_native = sdf_native.filter(F.col(\"id\") < LENGTH - LENGTH/5)\n",
    "sdf_native = sdf_native.select(\"id\", \"new_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75f1c6af-aff9-49e4-8453-e4c3f527a035",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sdf_native.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa52f335-b4fa-4a49-9ab6-e216f230e894",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%timeit sdf_native.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "124dd4bb-fa0a-4a7d-9f47-d782b01e0c7a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### scalar udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c06be65-3f7b-4cf7-b653-feaa84315c31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scalar_when_udf = F.udf(scalar_when, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42846878-bc7d-41c4-9342-b5c87d0f0681",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sdf_sclar = generate_dataset().to_spark()\n",
    "sdf_sclar = sdf_sclar.filter(F.col(\"id\") < 90)\n",
    "sdf_sclar = sdf_sclar.withColumn(\"new_value\", scalar_when_udf(F.col(\"id\")))\n",
    "sdf_sclar = sdf_sclar.filter(F.col(\"id\") < 80)\n",
    "sdf_sclar = sdf_sclar.select(\"id\", \"new_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cae3afa-8c1f-4791-87ee-79d0ed0384f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sdf_sclar.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f55398-93ca-4e47-acd1-6360dcec9265",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%timeit sdf_sclar.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f3c0203-012e-45c9-9b02-2a23fe536783",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### vect udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c817db97-d5db-4452-ba8c-265e21e302ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vect_when_udf = F.pandas_udf(vectorized_when_pd, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e71f0f9-dc47-4b9c-afcc-68b435236e74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sdf_vect = generate_dataset().to_spark()\n",
    "sdf_vect = sdf_vect.filter(F.col(\"id\") < 90)\n",
    "sdf_vect = sdf_vect.withColumn(\"new_value\", vect_when_udf(F.col(\"id\")))\n",
    "sdf_vect = sdf_vect.filter(F.col(\"id\") < 80)\n",
    "sdf_vect = sdf_vect.select(\"id\", \"new_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cc37aa4-7957-4872-87ef-d018813e7d85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sdf_vect.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sdf_vect.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "pyspark-pandas",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
