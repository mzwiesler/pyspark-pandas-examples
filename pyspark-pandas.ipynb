{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25bcdf6c-3c96-48f6-b7be-3a3721d3a161",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Prerequisites for spark and arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e596d735-c5b3-4b7f-a8b8-a176fc324607",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import LongType, StructField, StructType, DoubleType, IntegerType\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a99a81-1866-431f-9310-00be53ea3a05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/01 17:50:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# setup for local testing - comment in case of databricks\n",
    "builder = SparkSession.builder.master(\"local[4]\").appName(\"pandas-on-spark\")\n",
    "builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.ui.enabled\", \"false\")\n",
    "# Pandas API on Spark automatically uses this Spark session with the configurations set.\n",
    "spark = builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a21678d-1c8f-4dfe-a704-dfda9f9f4ad7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LENGTH = 500000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97616ae6-2299-47fa-8489-d3cfac9ed4db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Evaluate how to get pypsark schema for pandas pyspark dataframes\n",
    "\n",
    "Note that DataFrame.spark.schema() is not listed in the [API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.transform.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc036a9-0269-49d1-8d91-59710b445c6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "psdf = ps.DataFrame({\"id\": range(LENGTH), \"value\": range(10, LENGTH+10), \"a\": range(LENGTH,LENGTH*2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09036537-804e-445c-8ad3-84416f8a6861",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('id', LongType(), False), StructField('value', LongType(), False), StructField('a', LongType(), False)])\n"
     ]
    }
   ],
   "source": [
    "print(psdf.spark.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4efd813-d74b-4372-addb-e0ed7bc6a019",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- value: long (nullable = false)\n",
      " |-- a: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "sdf = psdf.to_spark()\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d3a905c-2d80-4dc3-b988-d5bc1debf23e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Test how to enforce nullable on schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "329973e3-f415-40a6-a526-71b3e04fe578",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_new = StructType(\n",
    "    [StructField(\"id\", LongType(), True), StructField(\"value\", LongType(), True), StructField('a', LongType(), False)]\n",
    ")\n",
    "sdf_new_schema = spark.createDataFrame(sdf.rdd, schema_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d8086f-edd6-4264-8ffc-c05a25467013",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      " |-- a: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_new_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fafb5b5-89c4-4c99-b0b3-72ec4463447a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', LongType(), True), StructField('value', LongType(), True), StructField('a', LongType(), False)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf_new_schema = sdf_new_schema.pandas_api()\n",
    "psdf_new_schema.spark.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e1e8c2b-785f-436a-9642-2ec4eb356ff3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# psdf_new_schema.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03105993-e1d2-48b3-b41c-27bf17ed5127",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Select and filter columns to pandas on pyspark df\n",
    "\n",
    "The goal is to compare the performance of three different approaches: native pyspark, pandas on pyspark with vectorized function and pyspark with scalar function. For this we do the same transformation in each approach and check the execution plan as well as the time taken to execute (on a larger dataset on databricks).\n",
    "\n",
    "The transformations should be two filters which could be optimized and one complex filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaca0007-f2b8-49f0-b6c2-268d5b27c7fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First we generally investigate how pandas dataframes are working in pyspark and see that we need to be careful how functions are applied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d13f507-c776-4975-8ba2-f8e262449e0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If the type hints is not specified for `apply`, it is expensive to infer the data type internally.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "A     249\n",
       "A     499\n",
       "A     749\n",
       "A    1000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.DataFrame({'A': range(1001)}).apply(lambda col: col.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3411e91f-32d2-456c-9766-7e2f722954ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Scalar UDFS vs pandas UDFs \n",
    "\n",
    "see [this post](https://gist.github.com/BryanCutler/0b0c820c1beb5ffc40618c462912195f) for more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5089a24-18aa-4b90-9a38-3ab97595dc13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Short excurse to pandas vs python udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc77cfa8-b594-4e43-94c5-86d688d41282",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.range(1 << 24, numPartitions=16).toDF(\"id\") \\\n",
    "        .withColumn(\"p1\", F.rand()).withColumn(\"p2\", F.rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "577476bf-36e7-488b-a3a1-b4cd13ee11ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "\n",
    "def scalar_func(p1, p2):\n",
    "    w = 0.5\n",
    "    return exp(log(p1) + log(p2) - log(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1a596c-268b-474e-acd8-d458ed2e5d3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:==========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.34 s ± 146 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "my_udf = F.udf(scalar_func, DoubleType())\n",
    "\n",
    "result = df.withColumn(\"p\", my_udf(F.col(\"p1\"), F.col(\"p2\")))\n",
    "\n",
    "%timeit result.filter(\"p < 1.0\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460e8fa9-17c3-4f7f-9c71-27bd4c863581",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def vect_func(p1, p2):\n",
    "    w = 0.5\n",
    "    return np.exp(np.log(p1) + np.log(p2) - np.log(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f12a5092-820f-453e-be46-00c2b2246099",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:======================================>                 (11 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.36 s ± 52.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "my_udf = F.pandas_udf(vect_func, DoubleType())\n",
    "\n",
    "result = df.withColumn(\"p\", my_udf(F.col(\"p1\"), F.col(\"p2\")))\n",
    "\n",
    "%timeit result.filter(\"p < 1.0\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6843ea0a-17fe-4c3b-9a4a-5493aa8785e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Start with execution of different when statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe8f42d8-96d8-4a9c-b63d-995116762c43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# scalar udf\n",
    "def scalar_when(x: int) -> int: \n",
    "    return 100 if x < LENGTH/20 else (50 if x < LENGTH/2 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb12bcb1-171d-4219-a35b-5a881fdedb72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# vectorized udf returning a pandas Series\n",
    "def vectorized_when_pd(x: pd.Series) -> pd.Series: \n",
    "  return pd.Series(np.where(x < LENGTH/20, 100, np.where(x < LENGTH/2, 50, 0)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized udf returning a numpy array\n",
    "def vectorized_when(x): \n",
    "  return np.where(x < LENGTH/20, 100, np.where(x < LENGTH/2, 50, 0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b269450-afa2-4736-aa68-06083c05748f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_dataset(len: int = LENGTH) -> ps.DataFrame:\n",
    "  return ps.DataFrame({\"id\": range(len), \"value\": range(10, len+10), \"a\": range(len,len*2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f97f873-ab51-456c-b668-23bd757cbe66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### With native pyspark pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe678b3-3c9c-434e-a5fb-fcb12efae8f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "psdf_native = generate_dataset()\n",
    "psdf_native = psdf_native.loc[psdf_native['id'] < (LENGTH - LENGTH/10)]\n",
    "psdf_native[\"new_value\"] = psdf_native[\"id\"].where(psdf_native[\"id\"] < LENGTH/20, 100).where(psdf_native[\"id\"] < LENGTH/2, 50).where(psdf_native[\"id\"] >= LENGTH/2, 0)\n",
    "psdf_native = psdf_native.loc[psdf_native['id'] < (LENGTH - LENGTH/5)]\n",
    "psdf_native = psdf_native.loc[:, ['new_value', 'id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef8683f3-9d5d-456c-b07e-0ba2e259bd9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "LocalTableScan (1)\n",
      "\n",
      "\n",
      "(1) LocalTableScan\n",
      "Output [3]: [__index_level_0__#287L, new_value#340L, id#288L]\n",
      "Arguments: [__index_level_0__#287L, new_value#340L, id#288L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "psdf_native.spark.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c92c7631-7b20-410d-ae6f-987621190008",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/01 17:51:14 WARN TaskSetManager: Stage 51 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:15 WARN TaskSetManager: Stage 54 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:17 WARN TaskSetManager: Stage 57 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:19 WARN TaskSetManager: Stage 60 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:20 WARN TaskSetManager: Stage 63 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:22 WARN TaskSetManager: Stage 66 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:23 WARN TaskSetManager: Stage 69 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.65 s ± 63 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/01 17:51:25 WARN TaskSetManager: Stage 72 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "%timeit psdf_native.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35250b63-c7fc-43fb-a8de-551ccb99ff54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Use pandas (vectorized) udf for pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6773d0-1e95-4771-9e5b-09c3c6295f0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The [apply and transform](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/transform_apply.html) functions are working on pandas series and thus have the same effect as using pandas udfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8c02d53-a36c-4768-b648-2d728f676003",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this runs a spark job - in case output type is not defined - since: this API executes the function once to infer the type which is potentially expensive, for instance, when the dataset is created after aggregations or sorting. (https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.transform.html)\n",
    "\n",
    "psdf_scalar = generate_dataset()\n",
    "psdf_scalar = psdf_scalar.loc[psdf_scalar['id'] < (LENGTH - LENGTH/10)]\n",
    "psdf_scalar = psdf_scalar.assign(new_value=psdf_scalar[\"id\"].transform(vectorized_when))\n",
    "psdf_scalar = psdf_scalar.loc[psdf_scalar['id'] < (LENGTH - LENGTH/5)]\n",
    "psdf_scalar = psdf_scalar.loc[:, ['new_value', 'id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf6acda-9a04-415e-9f20-fa899a6e3c6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Project (4)\n",
      "+- ArrowEvalPython (3)\n",
      "   +- * Project (2)\n",
      "      +- * LocalTableScan (1)\n",
      "\n",
      "\n",
      "(1) LocalTableScan [codegen id : 1]\n",
      "Output [5]: [__index_level_0__#693L, id#694L, value#695L, a#696L, __natural_order__#719L]\n",
      "Arguments: [__index_level_0__#693L, id#694L, value#695L, a#696L, __natural_order__#719L]\n",
      "\n",
      "(2) Project [codegen id : 1]\n",
      "Output [2]: [__index_level_0__#693L, id#694L]\n",
      "Input [5]: [__index_level_0__#693L, id#694L, value#695L, a#696L, __natural_order__#719L]\n",
      "\n",
      "(3) ArrowEvalPython\n",
      "Input [2]: [__index_level_0__#693L, id#694L]\n",
      "Arguments: [pudf(__index_level_0__#693L, id#694L)#747L], [pythonUDF0#775L], 200\n",
      "\n",
      "(4) Project [codegen id : 2]\n",
      "Output [3]: [__index_level_0__#693L, pythonUDF0#775L AS new_value#750L, id#694L]\n",
      "Input [3]: [__index_level_0__#693L, id#694L, pythonUDF0#775L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "psdf_scalar.spark.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6e3355a-509e-4fa3-aa0a-2c17715795c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/01 17:51:29 WARN TaskSetManager: Stage 76 contains a task of very large size (6355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:31 WARN TaskSetManager: Stage 79 contains a task of very large size (6355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:33 WARN TaskSetManager: Stage 82 contains a task of very large size (6355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:34 WARN TaskSetManager: Stage 85 contains a task of very large size (6355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:36 WARN TaskSetManager: Stage 88 contains a task of very large size (6355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:38 WARN TaskSetManager: Stage 91 contains a task of very large size (6355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:39 WARN TaskSetManager: Stage 94 contains a task of very large size (6355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:41 WARN TaskSetManager: Stage 97 contains a task of very large size (6355 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.72 s ± 55.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit psdf_scalar.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb78c89-88d9-475d-82c9-3f9579b8a509",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Use pandas (scalar) udf for pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1a26b97-d01d-4778-bf4c-45290fd881ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "psdf_vect = generate_dataset()\n",
    "psdf_vect = psdf_vect.loc[psdf_vect['id'] < (LENGTH - LENGTH/10)]\n",
    "psdf_vect = psdf_vect.assign(new_value=psdf_vect[\"id\"].transform(scalar_when))\n",
    "psdf_vect = psdf_vect.loc[psdf_vect['id'] < (LENGTH - LENGTH/5)]\n",
    "psdf_vect = psdf_vect.loc[:, ['new_value', 'id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f0bf293-8eac-4b23-aa71-95056bb04240",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Project (4)\n",
      "+- ArrowEvalPython (3)\n",
      "   +- * Project (2)\n",
      "      +- * LocalTableScan (1)\n",
      "\n",
      "\n",
      "(1) LocalTableScan [codegen id : 1]\n",
      "Output [5]: [__index_level_0__#1144L, id#1145L, value#1146L, a#1147L, __natural_order__#1170L]\n",
      "Arguments: [__index_level_0__#1144L, id#1145L, value#1146L, a#1147L, __natural_order__#1170L]\n",
      "\n",
      "(2) Project [codegen id : 1]\n",
      "Output [2]: [__index_level_0__#1144L, id#1145L]\n",
      "Input [5]: [__index_level_0__#1144L, id#1145L, value#1146L, a#1147L, __natural_order__#1170L]\n",
      "\n",
      "(3) ArrowEvalPython\n",
      "Input [2]: [__index_level_0__#1144L, id#1145L]\n",
      "Arguments: [pudf(__index_level_0__#1144L, id#1145L)#1178L], [pythonUDF0#1206L], 200\n",
      "\n",
      "(4) Project [codegen id : 2]\n",
      "Output [3]: [__index_level_0__#1144L, pythonUDF0#1206L AS new_value#1181L, id#1145L]\n",
      "Input [3]: [__index_level_0__#1144L, id#1145L, pythonUDF0#1206L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "psdf_vect.spark.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f101cd1-67ab-4b9f-abcb-3da8db279baf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/01 17:51:45 WARN TaskSetManager: Stage 100 contains a task of very large size (6355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:46 WARN TaskSetManager: Stage 103 contains a task of very large size (6355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:48 WARN TaskSetManager: Stage 106 contains a task of very large size (6355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:50 WARN TaskSetManager: Stage 109 contains a task of very large size (6355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:51 WARN TaskSetManager: Stage 112 contains a task of very large size (6355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:53 WARN TaskSetManager: Stage 115 contains a task of very large size (6355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:55 WARN TaskSetManager: Stage 118 contains a task of very large size (6355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:51:56 WARN TaskSetManager: Stage 121 contains a task of very large size (6355 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.63 s ± 104 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit psdf_vect.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad65c3b6-7193-4272-8615-a6c84f583a90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Native pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47194588-7f80-420d-b745-e63432619a01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "sdf_native = generate_dataset().to_spark()\n",
    "sdf_native = sdf_native.filter(F.col(\"id\") < LENGTH - LENGTH/10)\n",
    "sdf_native = sdf_native.withColumn(\"new_value\", F.when(F.col(\"id\") < 40, 100).when(F.col(\"id\") < 60, 50).otherwise(0))\n",
    "sdf_native = sdf_native.filter(F.col(\"id\") < LENGTH - LENGTH/5)\n",
    "sdf_native = sdf_native.select(\"id\", \"new_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75f1c6af-aff9-49e4-8453-e4c3f527a035",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "LocalTableScan (1)\n",
      "\n",
      "\n",
      "(1) LocalTableScan\n",
      "Output [2]: [id#1576L, new_value#1592]\n",
      "Arguments: [id#1576L, new_value#1592]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_native.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa52f335-b4fa-4a49-9ab6-e216f230e894",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/01 17:51:59 WARN TaskSetManager: Stage 124 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:52:00 WARN TaskSetManager: Stage 127 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:52:01 WARN TaskSetManager: Stage 130 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:52:01 WARN TaskSetManager: Stage 133 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:52:02 WARN TaskSetManager: Stage 136 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:52:03 WARN TaskSetManager: Stage 139 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 17:52:04 WARN TaskSetManager: Stage 142 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 ms ± 21.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/01 17:52:05 WARN TaskSetManager: Stage 145 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "%timeit sdf_native.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "124dd4bb-fa0a-4a7d-9f47-d782b01e0c7a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### scalar udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c06be65-3f7b-4cf7-b653-feaa84315c31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scalar_when_udf = F.udf(scalar_when, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42846878-bc7d-41c4-9342-b5c87d0f0681",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "sdf_sclar = generate_dataset().to_spark()\n",
    "sdf_sclar = sdf_sclar.filter(F.col(\"id\") < 90)\n",
    "sdf_sclar = sdf_sclar.withColumn(\"new_value\", scalar_when_udf(F.col(\"id\")))\n",
    "sdf_sclar = sdf_sclar.filter(F.col(\"id\") < 80)\n",
    "sdf_sclar = sdf_sclar.select(\"id\", \"new_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cae3afa-8c1f-4791-87ee-79d0ed0384f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Project (4)\n",
      "+- BatchEvalPython (3)\n",
      "   +- * Project (2)\n",
      "      +- * LocalTableScan (1)\n",
      "\n",
      "\n",
      "(1) LocalTableScan [codegen id : 1]\n",
      "Output [3]: [id#1656L, value#1657L, a#1658L]\n",
      "Arguments: [id#1656L, value#1657L, a#1658L]\n",
      "\n",
      "(2) Project [codegen id : 1]\n",
      "Output [1]: [id#1656L]\n",
      "Input [3]: [id#1656L, value#1657L, a#1658L]\n",
      "\n",
      "(3) BatchEvalPython\n",
      "Input [1]: [id#1656L]\n",
      "Arguments: [scalar_when(id#1656L)#1672L], [pythonUDF0#1680L]\n",
      "\n",
      "(4) Project [codegen id : 2]\n",
      "Output [2]: [id#1656L, pythonUDF0#1680L AS new_value#1673L]\n",
      "Input [2]: [id#1656L, pythonUDF0#1680L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_sclar.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f55398-93ca-4e47-acd1-6360dcec9265",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 ms ± 13.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sdf_sclar.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f3c0203-012e-45c9-9b02-2a23fe536783",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### vect udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c817db97-d5db-4452-ba8c-265e21e302ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vect_when_udf = F.pandas_udf(vectorized_when_pd, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e71f0f9-dc47-4b9c-afcc-68b435236e74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "sdf_vect = generate_dataset().to_spark()\n",
    "sdf_vect = sdf_vect.filter(F.col(\"id\") < 90)\n",
    "sdf_vect = sdf_vect.withColumn(\"new_value\", vect_when_udf(F.col(\"id\")))\n",
    "sdf_vect = sdf_vect.filter(F.col(\"id\") < 80)\n",
    "sdf_vect = sdf_vect.select(\"id\", \"new_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cc37aa4-7957-4872-87ef-d018813e7d85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Project (4)\n",
      "+- ArrowEvalPython (3)\n",
      "   +- * Project (2)\n",
      "      +- * LocalTableScan (1)\n",
      "\n",
      "\n",
      "(1) LocalTableScan [codegen id : 1]\n",
      "Output [3]: [id#1738L, value#1739L, a#1740L]\n",
      "Arguments: [id#1738L, value#1739L, a#1740L]\n",
      "\n",
      "(2) Project [codegen id : 1]\n",
      "Output [1]: [id#1738L]\n",
      "Input [3]: [id#1738L, value#1739L, a#1740L]\n",
      "\n",
      "(3) ArrowEvalPython\n",
      "Input [1]: [id#1738L]\n",
      "Arguments: [vectorized_when_pd(id#1738L)#1754], [pythonUDF0#1762], 200\n",
      "\n",
      "(4) Project [codegen id : 2]\n",
      "Output [2]: [id#1738L, pythonUDF0#1762 AS new_value#1755]\n",
      "Input [2]: [id#1738L, pythonUDF0#1762]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_vect.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383 ms ± 24.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sdf_vect.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "pyspark-pandas",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
