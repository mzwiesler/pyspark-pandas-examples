{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import LongType, StructField, StructType\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = SparkSession.builder.appName(\"pandas-on-spark\")\n",
    "builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "# Pandas API on Spark automatically uses this Spark session with the configurations set.\n",
    "spark = builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pandas on pyspark schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf = ps.DataFrame({\"id\": range(LENGTH), \"value\": range(10, LENGTH+10), \"a\": range(LENGTH,LENGTH*2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('id', LongType(), False), StructField('value', LongType(), False), StructField('a', LongType(), False)])\n"
     ]
    }
   ],
   "source": [
    "print(psdf.spark.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- value: long (nullable = false)\n",
      " |-- a: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "sdf = psdf.to_spark()\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_new = StructType(\n",
    "    [StructField(\"id\", LongType(), True), StructField(\"value\", LongType(), True), StructField('a', LongType(), False)]\n",
    ")\n",
    "sdf_new_schema = spark.createDataFrame(sdf.rdd, schema_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      " |-- a: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_new_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', LongType(), True), StructField('value', LongType(), True), StructField('a', LongType(), False)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf_new_schema = sdf_new_schema.pandas_api()\n",
    "psdf_new_schema.spark.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psdf_new_schema.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and filter columns to pandas on pyspark df\n",
    "\n",
    "The goal is to compare the performance of three different approaches: native pyspark, pandas on pyspark and pandas with pyarrow. For this we do the same transformation in each approach and check the execution plan as well as the time taken to execute (on a larger dataset on databricks).\n",
    "\n",
    "The transformations should be two filters which could be optimized and one complex filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_when(x: int) -> int: \n",
    "    return 100 if x < LENGTH/20 else (50 if x < LENGTH/2 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pandas():\n",
    "    pdf = pd.DataFrame({\"id\": range(LENGTH), \"value\": range(10, LENGTH+10), \"a\": range(LENGTH,LENGTH*2)}) \n",
    "    pdf = pdf.loc[pdf['id'] < LENGTH-LENGTH/10]\n",
    "    pdf[\"new_value\"] = pdf[\"id\"].transform(multi_when)\n",
    "    pdf = pdf.loc[pdf['id'] < LENGTH-LENGTH/5]\n",
    "    pdf = pdf.loc[:, ['new_value', 'id']]\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_value</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799995</th>\n",
       "      <td>0</td>\n",
       "      <td>799995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799996</th>\n",
       "      <td>0</td>\n",
       "      <td>799996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799997</th>\n",
       "      <td>0</td>\n",
       "      <td>799997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799998</th>\n",
       "      <td>0</td>\n",
       "      <td>799998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799999</th>\n",
       "      <td>0</td>\n",
       "      <td>799999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        new_value      id\n",
       "0             100       0\n",
       "1             100       1\n",
       "2             100       2\n",
       "3             100       3\n",
       "4             100       4\n",
       "...           ...     ...\n",
       "799995          0  799995\n",
       "799996          0  799996\n",
       "799997          0  799997\n",
       "799998          0  799998\n",
       "799999          0  799999\n",
       "\n",
       "[800000 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = compute_pandas()\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf_select_filter = ps.DataFrame({\"id\": range(LENGTH), \"value\": range(10, LENGTH+10), \"a\": range(LENGTH,LENGTH*2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [apply and transform](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/transform_apply.html) functions are working on pandas series and thus have the same effect as using pandas udfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf_select_filter = psdf_select_filter.loc[psdf_select_filter['id'] < (LENGTH - LENGTH/10)]\n",
    "psdf_select_filter[\"new_value\"] = psdf_select_filter[\"id\"].transform(multi_when)\n",
    "psdf_select_filter = psdf_select_filter.loc[psdf_select_filter['id'] < (LENGTH - LENGTH/5)]\n",
    "psdf_select_filter = psdf_select_filter.loc[:, ['new_value', 'id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Project (4)\n",
      "+- ArrowEvalPython (3)\n",
      "   +- * Project (2)\n",
      "      +- * LocalTableScan (1)\n",
      "\n",
      "\n",
      "(1) LocalTableScan [codegen id : 1]\n",
      "Output [5]: [__index_level_0__#50L, id#51L, value#52L, a#53L, __natural_order__#76L]\n",
      "Arguments: [__index_level_0__#50L, id#51L, value#52L, a#53L, __natural_order__#76L]\n",
      "\n",
      "(2) Project [codegen id : 1]\n",
      "Output [2]: [__index_level_0__#50L, id#51L]\n",
      "Input [5]: [__index_level_0__#50L, id#51L, value#52L, a#53L, __natural_order__#76L]\n",
      "\n",
      "(3) ArrowEvalPython\n",
      "Input [2]: [__index_level_0__#50L, id#51L]\n",
      "Arguments: [pudf(__index_level_0__#50L, id#51L)#84L], [pythonUDF0#112L], 200\n",
      "\n",
      "(4) Project [codegen id : 2]\n",
      "Output [3]: [__index_level_0__#50L, pythonUDF0#112L AS new_value#87L, id#51L]\n",
      "Input [3]: [__index_level_0__#50L, id#51L, pythonUDF0#112L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "psdf_select_filter.spark.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/01 12:40:41 WARN TaskSetManager: Stage 0 contains a task of very large size (3181 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/01 12:40:50 WARN TaskMemoryManager: Failed to allocate a page (1048560 bytes), try again.\n",
      "24/08/01 12:40:50 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" 24/08/01 12:40:50 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:40:50 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" 24/08/01 12:40:50 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:40:51 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:40:51 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1291, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:40:52 WARN ArrowPythonRunner: Detected deadlock while completing task 5.0 in stage 0 (TID 5): Attempting to kill Python Worker\n",
      "24/08/01 12:40:52 WARN ArrowPythonRunner: Detected deadlock while completing task 11.0 in stage 0 (TID 11): Attempting to kill Python Worker\n",
      "24/08/01 12:40:53 WARN TaskMemoryManager: Failed to allocate a page (1048560 bytes), try again.\n",
      "24/08/01 12:40:53 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread \"stdout writer for python3\"\n",
      "24/08/01 12:40:56 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:40:56 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:40:56 WARN TaskMemoryManager: Failed to allocate a page (1048560 bytes), try again.\n",
      "24/08/01 12:40:56 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread \"RemoteBlock-temp-file-clean-thread\"\n",
      "24/08/01 12:41:00 WARN TaskMemoryManager: Failed to allocate a page (1048560 bytes), try again.\n",
      "24/08/01 12:41:00 WARN TaskMemoryManager: Failed to allocate a page (1048560 bytes), try again.\n",
      "24/08/01 12:41:00 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:41:00 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread \"refresh progress\"\n",
      "24/08/01 12:41:03 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:41:03 ERROR Utils: Uncaught exception in thread driver-heartbeater\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:41:03 ERROR Utils: uncaught error in thread Spark Context Cleaner, stopping SparkContext\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:41:03 WARN TaskMemoryManager: Failed to allocate a page (1048560 bytes), try again.\n",
      "\n",
      "Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread \"Spark Context Cleaner\"\n",
      "Exception in thread \"executor-heartbeater\" Exception in thread \"executor-kill-mark-cleanup\" java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"Executor task launch worker for task 13.0 in stage 0.0 (TID 13)\" java.lang.OutOfMemoryError: Java heap space\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:41:12 WARN TaskMemoryManager: Failed to allocate a page (1048560 bytes), try again.\n",
      "24/08/01 12:41:12 ERROR Executor: Exception in task 14.0 in stage 0.0 (TID 14)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:41:12 ERROR Executor: Exception in task 3.0 in stage 0.0 (TID 3)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:41:12 ERROR Executor: Exception in task 5.0 in stage 0.0 (TID 5)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"Executor task launch worker for task 9.0 in stage 0.0 (TID 9)\" java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"Executor task launch worker for task 11.0 in stage 0.0 (TID 11)\" java.lang.InternalError: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.invoke.InnerClassLambdaMetafactory.generateInnerClass(InnerClassLambdaMetafactory.java:413)\n",
      "\tat java.base/java.lang.invoke.InnerClassLambdaMetafactory.spinInnerClass(InnerClassLambdaMetafactory.java:315)\n",
      "\tat java.base/java.lang.invoke.InnerClassLambdaMetafactory.buildCallSite(InnerClassLambdaMetafactory.java:228)\n",
      "\tat java.base/java.lang.invoke.LambdaMetafactory.altMetafactory(LambdaMetafactory.java:536)\n",
      "\tat java.base/java.lang.invoke.BootstrapMethodInvoker.invoke(BootstrapMethodInvoker.java:147)\n",
      "\tat java.base/java.lang.invoke.CallSite.makeSite(CallSite.java:315)\n",
      "\tat java.base/java.lang.invoke.MethodHandleNatives.linkCallSiteImpl(MethodHandleNatives.java:281)\n",
      "\tat java.base/java.lang.invoke.MethodHandleNatives.linkCallSite(MethodHandleNatives.java:271)\n",
      "\tat org.apache.spark.executor.ExecutorMetricsPoller.decrementCount$1(ExecutorMetricsPoller.scala:133)\n",
      "\tat org.apache.spark.executor.ExecutorMetricsPoller.$anonfun$onTaskCompletion$3(ExecutorMetricsPoller.scala:138)\n",
      "\tat java.base/java.util.concurrent.ConcurrentHashMap.computeIfPresent(ConcurrentHashMap.java:1828)\n",
      "\tat org.apache.spark.executor.ExecutorMetricsPoller.onTaskCompletion(ExecutorMetricsPoller.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:834)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:41:12 WARN Utils: Suppressing exception in finally: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "java.lang.NoClassDefFoundError: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "\tat org.sparkproject.guava.collect.AbstractIterator.<init>(AbstractIterator.java:65)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset$2.<init>(ConcurrentHashMultiset.java:497)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset.entryIterator(ConcurrentHashMultiset.java:496)\n",
      "\tat org.sparkproject.guava.collect.AbstractMultiset$EntrySet.iterator(AbstractMultiset.java:177)\n",
      "\tat java.base/java.lang.Iterable.forEach(Iterable.java:74)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:493)\n",
      "\tat org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:1326)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:624)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:73)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "24/08/01 12:41:12 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)\n",
      "java.lang.IllegalArgumentException: Self-suppression not permitted\n",
      "\tat java.base/java.lang.Throwable.addSuppressed(Throwable.java:1072)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\tSuppressed: java.lang.NoClassDefFoundError: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "\t\tat org.sparkproject.guava.collect.AbstractIterator.<init>(AbstractIterator.java:65)\n",
      "\t\tat org.sparkproject.guava.collect.ConcurrentHashMultiset$2.<init>(ConcurrentHashMultiset.java:497)\n",
      "\t\tat org.sparkproject.guava.collect.ConcurrentHashMultiset.entryIterator(ConcurrentHashMultiset.java:496)\n",
      "\t\tat org.sparkproject.guava.collect.AbstractMultiset$EntrySet.iterator(AbstractMultiset.java:177)\n",
      "\t\tat java.base/java.lang.Iterable.forEach(Iterable.java:74)\n",
      "\t\tat org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:493)\n",
      "\t\tat org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:1326)\n",
      "\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:624)\n",
      "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:73)\n",
      "\t\t... 6 more\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:41:12 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 3.0 in stage 0.0 (TID 3),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:41:12 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 5.0 in stage 0.0 (TID 5),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:41:12 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 14.0 in stage 0.0 (TID 14),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:41:12 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 0.0 (TID 1),5,main]\n",
      "java.lang.IllegalArgumentException: Self-suppression not permitted\n",
      "\tat java.base/java.lang.Throwable.addSuppressed(Throwable.java:1072)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\tSuppressed: java.lang.NoClassDefFoundError: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "\t\tat org.sparkproject.guava.collect.AbstractIterator.<init>(AbstractIterator.java:65)\n",
      "\t\tat org.sparkproject.guava.collect.ConcurrentHashMultiset$2.<init>(ConcurrentHashMultiset.java:497)\n",
      "\t\tat org.sparkproject.guava.collect.ConcurrentHashMultiset.entryIterator(ConcurrentHashMultiset.java:496)\n",
      "\t\tat org.sparkproject.guava.collect.AbstractMultiset$EntrySet.iterator(AbstractMultiset.java:177)\n",
      "\t\tat java.base/java.lang.Iterable.forEach(Iterable.java:74)\n",
      "\t\tat org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:493)\n",
      "\t\tat org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:1326)\n",
      "\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:624)\n",
      "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:73)\n",
      "\t\t... 6 more\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/01 12:41:13 WARN TaskSetManager: Lost task 5.0 in stage 0.0 (TID 5) (192.168.2.100 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "24/08/01 12:41:13 ERROR TaskSetManager: Task 5 in stage 0.0 failed 1 times; aborting job\n",
      "24/08/01 12:41:13 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1) (192.168.2.100 executor driver): java.lang.IllegalArgumentException: Self-suppression not permitted\n",
      "\tat java.base/java.lang.Throwable.addSuppressed(Throwable.java:1072)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\tSuppressed: java.lang.NoClassDefFoundError: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "\t\tat org.sparkproject.guava.collect.AbstractIterator.<init>(AbstractIterator.java:65)\n",
      "\t\tat org.sparkproject.guava.collect.ConcurrentHashMultiset$2.<init>(ConcurrentHashMultiset.java:497)\n",
      "\t\tat org.sparkproject.guava.collect.ConcurrentHashMultiset.entryIterator(ConcurrentHashMultiset.java:496)\n",
      "\t\tat org.sparkproject.guava.collect.AbstractMultiset$EntrySet.iterator(AbstractMultiset.java:177)\n",
      "\t\tat java.base/java.lang.Iterable.forEach(Iterable.java:74)\n",
      "\t\tat org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:493)\n",
      "\t\tat org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:1326)\n",
      "\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:624)\n",
      "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:73)\n",
      "\t\t... 6 more\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "24/08/01 12:41:13 WARN Utils: Suppressing exception in finally: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "java.lang.NoClassDefFoundError: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "\tat org.sparkproject.guava.collect.AbstractIterator.<init>(AbstractIterator.java:65)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset$2.<init>(ConcurrentHashMultiset.java:497)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset.entryIterator(ConcurrentHashMultiset.java:496)\n",
      "\tat org.sparkproject.guava.collect.AbstractMultiset$EntrySet.iterator(AbstractMultiset.java:177)\n",
      "\tat java.base/java.lang.Iterable.forEach(Iterable.java:74)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:493)\n",
      "\tat org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:1326)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:624)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:73)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "24/08/01 12:41:13 WARN Utils: Suppressing exception in finally: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "java.lang.NoClassDefFoundError: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "\tat org.sparkproject.guava.collect.AbstractIterator.<init>(AbstractIterator.java:65)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset$2.<init>(ConcurrentHashMultiset.java:497)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset.entryIterator(ConcurrentHashMultiset.java:496)\n",
      "\tat org.sparkproject.guava.collect.AbstractMultiset$EntrySet.iterator(AbstractMultiset.java:177)\n",
      "\tat java.base/java.lang.Iterable.forEach(Iterable.java:74)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:493)\n",
      "\tat org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:1326)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:624)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:73)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "24/08/01 12:41:13 WARN Utils: Suppressing exception in finally: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "java.lang.NoClassDefFoundError: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "\tat org.sparkproject.guava.collect.AbstractIterator.<init>(AbstractIterator.java:65)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset$2.<init>(ConcurrentHashMultiset.java:497)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset.entryIterator(ConcurrentHashMultiset.java:496)\n",
      "\tat org.sparkproject.guava.collect.AbstractMultiset$EntrySet.iterator(AbstractMultiset.java:177)\n",
      "\tat java.base/java.lang.Iterable.forEach(Iterable.java:74)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:493)\n",
      "\tat org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:1326)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:624)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:73)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "24/08/01 12:41:13 WARN Utils: Suppressing exception in finally: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "java.lang.NoClassDefFoundError: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "\tat org.sparkproject.guava.collect.AbstractIterator.<init>(AbstractIterator.java:65)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset$2.<init>(ConcurrentHashMultiset.java:497)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset.entryIterator(ConcurrentHashMultiset.java:496)\n",
      "\tat org.sparkproject.guava.collect.AbstractMultiset$EntrySet.iterator(AbstractMultiset.java:177)\n",
      "\tat java.base/java.lang.Iterable.forEach(Iterable.java:74)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:493)\n",
      "\tat org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:1326)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:624)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:73)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "24/08/01 12:41:13 WARN Utils: Suppressing exception in finally: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "java.lang.NoClassDefFoundError: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "\tat org.sparkproject.guava.collect.AbstractIterator.<init>(AbstractIterator.java:65)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset$2.<init>(ConcurrentHashMultiset.java:497)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset.entryIterator(ConcurrentHashMultiset.java:496)\n",
      "\tat org.sparkproject.guava.collect.AbstractMultiset$EntrySet.iterator(AbstractMultiset.java:177)\n",
      "\tat java.base/java.lang.Iterable.forEach(Iterable.java:74)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:493)\n",
      "\tat org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:1326)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:624)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:73)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "24/08/01 12:41:13 WARN Utils: Suppressing exception in finally: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "java.lang.NoClassDefFoundError: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "\tat org.sparkproject.guava.collect.AbstractIterator.<init>(AbstractIterator.java:65)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset$2.<init>(ConcurrentHashMultiset.java:497)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset.entryIterator(ConcurrentHashMultiset.java:496)\n",
      "\tat org.sparkproject.guava.collect.AbstractMultiset$EntrySet.iterator(AbstractMultiset.java:177)\n",
      "\tat java.base/java.lang.Iterable.forEach(Iterable.java:74)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:493)\n",
      "\tat org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:1326)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:624)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:73)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "24/08/01 12:41:13 WARN Utils: Suppressing exception in finally: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "java.lang.NoClassDefFoundError: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "\tat org.sparkproject.guava.collect.AbstractIterator.<init>(AbstractIterator.java:65)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset$2.<init>(ConcurrentHashMultiset.java:497)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset.entryIterator(ConcurrentHashMultiset.java:496)\n",
      "\tat org.sparkproject.guava.collect.AbstractMultiset$EntrySet.iterator(AbstractMultiset.java:177)\n",
      "\tat java.base/java.lang.Iterable.forEach(Iterable.java:74)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:493)\n",
      "\tat org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:1326)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:624)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:73)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "24/08/01 12:41:13 WARN Utils: Suppressing exception in finally: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "java.lang.NoClassDefFoundError: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "\tat org.sparkproject.guava.collect.AbstractIterator.<init>(AbstractIterator.java:65)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset$2.<init>(ConcurrentHashMultiset.java:497)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset.entryIterator(ConcurrentHashMultiset.java:496)\n",
      "\tat org.sparkproject.guava.collect.AbstractMultiset$EntrySet.iterator(AbstractMultiset.java:177)\n",
      "\tat java.base/java.lang.Iterable.forEach(Iterable.java:74)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:493)\n",
      "\tat org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:1326)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:624)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:73)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "24/08/01 12:41:13 WARN Utils: Suppressing exception in finally: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "java.lang.NoClassDefFoundError: Could not initialize class org.sparkproject.guava.collect.AbstractIterator$State\n",
      "\tat org.sparkproject.guava.collect.AbstractIterator.<init>(AbstractIterator.java:65)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset$2.<init>(ConcurrentHashMultiset.java:497)\n",
      "\tat org.sparkproject.guava.collect.ConcurrentHashMultiset.entryIterator(ConcurrentHashMultiset.java:496)\n",
      "\tat org.sparkproject.guava.collect.AbstractMultiset$EntrySet.iterator(AbstractMultiset.java:177)\n",
      "\tat java.base/java.lang.Iterable.forEach(Iterable.java:74)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:493)\n",
      "\tat org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:1326)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:624)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:73)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:198: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.pyspark.fallback.enabled' does not have an effect on failures in the middle of computation.\n",
      "  py4j does not exist in the JVM\n",
      "  warn(msg)\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/config.py:436\u001b[0m, in \u001b[0;36moption_context\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    435\u001b[0m         set_option(key, value)\n\u001b[0;32m--> 436\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/frame.py:810\u001b[0m, in \u001b[0;36mDataFrame._reduce_for_stat_function\u001b[0;34m(self, sfun, name, axis, numeric_only, skipna, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m         internal \u001b[38;5;241m=\u001b[39m InternalFrame(\n\u001b[1;32m    805\u001b[0m             spark_frame\u001b[38;5;241m=\u001b[39msdf,\n\u001b[1;32m    806\u001b[0m             index_spark_columns\u001b[38;5;241m=\u001b[39m[scol_for(sdf, SPARK_DEFAULT_INDEX_NAME)],\n\u001b[1;32m    807\u001b[0m             column_labels\u001b[38;5;241m=\u001b[39mnew_column_labels,\n\u001b[1;32m    808\u001b[0m             column_label_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal\u001b[38;5;241m.\u001b[39mcolumn_label_names,\n\u001b[1;32m    809\u001b[0m         )\n\u001b[0;32m--> 810\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m first_series(\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43minternal\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;66;03m# Here we execute with the first 1000 to get the return type.\u001b[39;00m\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;66;03m# If the records were less than 1000, it uses pandas API directly for a shortcut.\u001b[39;00m\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/frame.py:2722\u001b[0m, in \u001b[0;36mDataFrame.transpose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_compute_count \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2722\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_compute_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_internal_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pdf) \u001b[38;5;241m>\u001b[39m max_compute_count:\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/frame.py:13388\u001b[0m, in \u001b[0;36mDataFrame._to_internal_pandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  13383\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m  13384\u001b[0m \u001b[38;5;124;03mReturn a pandas DataFrame directly from _internal to avoid overhead of copy.\u001b[39;00m\n\u001b[1;32m  13385\u001b[0m \n\u001b[1;32m  13386\u001b[0m \u001b[38;5;124;03mThis method is for internal use only.\u001b[39;00m\n\u001b[1;32m  13387\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m> 13388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas_frame\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/utils.py:600\u001b[0m, in \u001b[0;36mlazy_property.<locals>.wrapped_lazy_property\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name):\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name, \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name)\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/internal.py:1115\u001b[0m, in \u001b[0;36mInternalFrame.to_pandas_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m sdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_internal_spark_frame\n\u001b[0;32m-> 1115\u001b[0m pdf \u001b[38;5;241m=\u001b[39m \u001b[43msdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pdf) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sdf\u001b[38;5;241m.\u001b[39mschema) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:131\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m self_destruct \u001b[38;5;241m=\u001b[39m jconf\u001b[38;5;241m.\u001b[39marrowPySparkSelfDestructEnabled()\n\u001b[0;32m--> 131\u001b[0m batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_as_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_destruct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batches) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:284\u001b[0m, in \u001b[0;36mPandasConversionMixin._collect_as_arrow\u001b[0;34m(self, split_batches)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m unwrap_spark_exception():\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;66;03m# Join serving thread and raise any exceptions from collectAsArrowToPython\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m         \u001b[43mjsocket_auth_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Separate RecordBatches from batch order indices in results\u001b[39;00m\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:181\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 181\u001b[0m     converted \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:173\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PythonException(msg, stacktrace)\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnknownException\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstackTrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacktrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:65\u001b[0m, in \u001b[0;36mCapturedException.__init__\u001b[0;34m(self, desc, stackTrace, cause, origin)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstackTrace \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     61\u001b[0m     stackTrace\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stackTrace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (SparkContext\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mUtils\u001b[38;5;241m.\u001b[39mexceptionString(origin))\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin\u001b[38;5;241m.\u001b[39mgetCause() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:173\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PythonException(msg, stacktrace)\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnknownException\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstackTrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacktrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:65\u001b[0m, in \u001b[0;36mCapturedException.__init__\u001b[0;34m(self, desc, stackTrace, cause, origin)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstackTrace \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     61\u001b[0m     stackTrace\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stackTrace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (SparkContext\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mUtils\u001b[38;5;241m.\u001b[39mexceptionString(origin))\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin\u001b[38;5;241m.\u001b[39mgetCause() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:136\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StreamingQueryException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_instance_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.sql.execution.QueryExecutionException\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QueryExecutionException(origin\u001b[38;5;241m=\u001b[39me)\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:464\u001b[0m, in \u001b[0;36mis_instance_of\u001b[0;34m(gateway, java_object, java_class)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava_class must be a string, a JavaClass, or a JavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgateway\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy4j\u001b[49m\u001b[38;5;241m.\u001b[39mreflection\u001b[38;5;241m.\u001b[39mTypeUtil\u001b[38;5;241m.\u001b[39misInstanceOf(\n\u001b[1;32m    465\u001b[0m     param, java_object)\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1725\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1723\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1725\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[0;31mPy4JError\u001b[0m: py4j does not exist in the JVM",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpsdf_select_filter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/generic.py:1795\u001b[0m, in \u001b[0;36mFrame.count\u001b[0;34m(self, axis, numeric_only)\u001b[0m\n\u001b[1;32m   1725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\n\u001b[1;32m   1726\u001b[0m     \u001b[38;5;28mself\u001b[39m, axis: Optional[Axis] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, numeric_only: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1727\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Scalar, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;124;03m    Count non-NA cells for each column.\u001b[39;00m\n\u001b[1;32m   1730\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;124;03m    4\u001b[39;00m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce_for_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_expr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/frame.py:803\u001b[0m, in \u001b[0;36mDataFrame._reduce_for_stat_function\u001b[0;34m(self, sfun, name, axis, numeric_only, skipna, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m sdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal\u001b[38;5;241m.\u001b[39mspark_frame\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;241m*\u001b[39mexprs)\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# The data is expected to be small so it's fine to transpose/use the default index.\u001b[39;00m\n\u001b[0;32m--> 803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ps\u001b[38;5;241m.\u001b[39moption_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute.max_rows\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    804\u001b[0m     internal \u001b[38;5;241m=\u001b[39m InternalFrame(\n\u001b[1;32m    805\u001b[0m         spark_frame\u001b[38;5;241m=\u001b[39msdf,\n\u001b[1;32m    806\u001b[0m         index_spark_columns\u001b[38;5;241m=\u001b[39m[scol_for(sdf, SPARK_DEFAULT_INDEX_NAME)],\n\u001b[1;32m    807\u001b[0m         column_labels\u001b[38;5;241m=\u001b[39mnew_column_labels,\n\u001b[1;32m    808\u001b[0m         column_label_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal\u001b[38;5;241m.\u001b[39mcolumn_label_names,\n\u001b[1;32m    809\u001b[0m     )\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m first_series(DataFrame(internal)\u001b[38;5;241m.\u001b[39mtranspose())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    151\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/config.py:439\u001b[0m, in \u001b[0;36moption_context\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m orig_opts\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 439\u001b[0m         \u001b[43mset_option\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/config.py:390\u001b[0m, in \u001b[0;36mset_option\u001b[0;34m(key, value)\u001b[0m\n\u001b[1;32m    388\u001b[0m _check_option(key)\n\u001b[1;32m    389\u001b[0m _options_dict[key]\u001b[38;5;241m.\u001b[39mvalidate(value)\n\u001b[0;32m--> 390\u001b[0m spark_session \u001b[38;5;241m=\u001b[39m \u001b[43mdefault_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m spark_session\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(_key_format(key), json\u001b[38;5;241m.\u001b[39mdumps(value))\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/utils.py:481\u001b[0m, in \u001b[0;36mdefault_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_session\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SparkSession:\n\u001b[0;32m--> 481\u001b[0m     spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetActiveSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spark \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m         spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas-on-Spark\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/sql/utils.py:280\u001b[0m, in \u001b[0;36mtry_remote_session_classmethod.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(SparkSession, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs[\u001b[38;5;241m1\u001b[39m:], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:680\u001b[0m, in \u001b[0;36mSparkSession.getActiveSession\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkSession\u001b[49m\u001b[38;5;241m.\u001b[39mgetActiveSession()\u001b[38;5;241m.\u001b[39misDefined():\n\u001b[1;32m    681\u001b[0m         SparkSession(sc, sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSparkSession\u001b[38;5;241m.\u001b[39mgetActiveSession()\u001b[38;5;241m.\u001b[39mget())\n\u001b[1;32m    682\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m SparkSession\u001b[38;5;241m.\u001b[39m_activeSession\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "psdf_select_filter.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.75 ns Â± 0.0349 ns per loop (mean Â± std. dev. of 7 runs, 100,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit psdf_select_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing the same for native pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sdf_select_filter \u001b[38;5;241m=\u001b[39m \u001b[43mps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m110\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_spark()\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/frame.py:573\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    570\u001b[0m         index \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39m_to_pandas()\n\u001b[1;32m    572\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mdata, index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m--> 573\u001b[0m     internal \u001b[38;5;241m=\u001b[39m \u001b[43mInternalFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m     index_assigned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m index_assigned:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;66;03m# TODO(SPARK-40226): Support MultiIndex\u001b[39;00m\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/internal.py:1520\u001b[0m, in \u001b[0;36mInternalFrame.from_pandas\u001b[0;34m(pdf)\u001b[0m\n\u001b[1;32m   1514\u001b[0m     column_labels \u001b[38;5;241m=\u001b[39m [(col,) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns]\n\u001b[1;32m   1516\u001b[0m column_label_names: List[Optional[Label]] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1517\u001b[0m     name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (name,) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m columns\u001b[38;5;241m.\u001b[39mnames\n\u001b[1;32m   1518\u001b[0m ]\n\u001b[0;32m-> 1520\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m \u001b[43mis_timestamp_ntz_preferred\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m (\n\u001b[1;32m   1523\u001b[0m     pdf,\n\u001b[1;32m   1524\u001b[0m     index_columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1527\u001b[0m     data_fields,\n\u001b[1;32m   1528\u001b[0m ) \u001b[38;5;241m=\u001b[39m InternalFrame\u001b[38;5;241m.\u001b[39mprepare_pandas_frame(pdf, prefer_timestamp_ntz\u001b[38;5;241m=\u001b[39mprefer_timestamp_ntz)\n\u001b[1;32m   1530\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([field\u001b[38;5;241m.\u001b[39mstruct_field \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m index_fields \u001b[38;5;241m+\u001b[39m data_fields])\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/sql/utils.py:153\u001b[0m, in \u001b[0;36mis_timestamp_ntz_preferred\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_jvm\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonSQLUtils\u001b[49m\u001b[38;5;241m.\u001b[39misTimestampNTZPreferred()\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "sdf_select_filter = ps.DataFrame({\"id\": range(100), \"value\": range(10, 110), \"a\": range(100,200)}).to_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- value: long (nullable = false)\n",
      " |-- a: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_select_filter.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_select_filter = sdf_select_filter.filter(F.col(\"id\") < LENGTH - LENGTH/10)\n",
    "sdf_select_filter = sdf_select_filter.withColumn(\"new_value\", F.when(F.col(\"id\") < 40, 100).when(F.col(\"id\") < 60, 50).otherwise(0))\n",
    "sdf_select_filter = sdf_select_filter.filter(F.col(\"id\") < LENGTH - LENGTH/5)\n",
    "sdf_select_filter = sdf_select_filter.select(\"id\", \"new_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "LocalTableScan (1)\n",
      "\n",
      "\n",
      "(1) LocalTableScan\n",
      "Output [2]: [id#7267L, new_value#7857]\n",
      "Arguments: [id#7267L, new_value#7857]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_select_filter.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.9 ms Â± 493 Î¼s per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sdf_select_filter.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing the same without arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzwiesl/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "sdf_select_filter2 = ps.DataFrame({\"id\": range(LENGTH), \"value\": range(10, LENGTH+10), \"a\": range(LENGTH,LENGTH*2)}).to_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_when_udf = F.udf(multi_when, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_select_filter2 = sdf_select_filter2.filter(F.col(\"id\") < 90)\n",
    "sdf_select_filter2 = sdf_select_filter2.withColumn(\"new_value\", multi_when_udf(F.col(\"id\")))\n",
    "sdf_select_filter2 = sdf_select_filter2.filter(F.col(\"id\") < 80)\n",
    "sdf_select_filter2 = sdf_select_filter2.select(\"id\", \"new_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.sql.api.python.PythonSQLUtils.explainString.\n: java.lang.OutOfMemoryError: Java heap space\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[191], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msdf_select_filter2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformatted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:741\u001b[0m, in \u001b[0;36mDataFrame.explain\u001b[0;34m(self, extended, mode)\u001b[0m\n\u001b[1;32m    739\u001b[0m     explain_mode \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, extended)\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 741\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonSQLUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplainString\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryExecution\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplain_mode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Repos/Free/pyspark-examples/.venv/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.sql.api.python.PythonSQLUtils.explainString.\n: java.lang.OutOfMemoryError: Java heap space\n"
     ]
    }
   ],
   "source": [
    "sdf_select_filter2.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+\n",
      "| id|new_value|\n",
      "+---+---------+\n",
      "|  0|      100|\n",
      "|  1|      100|\n",
      "|  2|      100|\n",
      "|  3|      100|\n",
      "|  4|      100|\n",
      "|  5|      100|\n",
      "|  6|      100|\n",
      "|  7|      100|\n",
      "|  8|      100|\n",
      "|  9|      100|\n",
      "| 10|      100|\n",
      "| 11|      100|\n",
      "| 12|      100|\n",
      "| 13|      100|\n",
      "| 14|      100|\n",
      "| 15|      100|\n",
      "| 16|      100|\n",
      "| 17|      100|\n",
      "| 18|      100|\n",
      "| 19|      100|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "117 ms Â± 1.27 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sdf_select_filter2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
